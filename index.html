<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="A new approach for self-supervised audio-visual separation using hierarchical fusion and representation alignment.">
  <meta property="og:title" content="Audio-Visual Separation with Hierarchical Fusion and Representation Alignment"/>
  <meta property="og:description" content="This work proposes a novel method for audio-visual source separation by integrating a hierarchical fusion strategy and a representation alignment approach."/>
  <meta property="og:url" content="https://happy-new-bears.github.io/AVSep-HFRA"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Audio-Visual Separation with Hierarchical Fusion and Representation Alignment">
  <meta name="twitter:description" content="A new approach for self-supervised audio-visual separation using hierarchical fusion and representation alignment.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="audio-visual separation, deep learning, hierarchical fusion, representation alignment, self-supervised learning, BMVC">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Audio-Visual Separation with Hierarchical Fusion and Representation Alignment</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Audio-Visual Separation with Hierarchical Fusion and Representation Alignment</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Han Hu</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Dongheng Lin</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Qiming Huang</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Yuqi Hou</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Hyung Jin Chang</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Jianbo Jiao</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">School of Computer Science, University of Birmingham<br>BMVC 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/BMVC_Han_final.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/happy-new-bears/AVSep-HFRA" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-supervised audio-visual source separation leverages natural correlations between audio and vision modalities to separate mixed audio signals. In this work, we first systematically analyze the performance of existing multimodal fusion methods for audio-visual separation task, demonstrating that the performance of different fusion strategies is closely linked to the characteristics of the sound-middle fusion is better suited for handling short, transient sounds, while late fusion is more effective for capturing sustained and harmonically rich sounds. We thus propose a hierarchical fusion strategy that effectively integrates both fusion stages. In addition, training can be made easier by incorporating high-quality external audio representations, rather than relying solely on the audio branch to learn them independently. To explore this, we propose a representation alignment approach that aligns the latent features of the audio encoder with embeddings extracted from pre-trained audio models. Extensive experiments on MUSIC, MUSIC-21 and VGGSound datasets demonstrate that our approach achieves state-of-the-art results, surpassing existing methods under the self-supervised setting. We further analyze the impact of representation alignment on audio features, showing that it reduces modality gap between the audio and visual modalities. The project page is at: <a href="https://github.com/happy-new-bears/AVSep-HFRA">https://github.com/happy-new-bears/AVSep-HFRA</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Model Overview</h2>
    <div class="columns is-centered">
      <div class="column">
        <img src="static/images/figure2.png" alt="Model Overview" style="width:100%;">
        <h2 class="subtitle has-text-centered">
          The overall architecture of our proposed method.
        </h2>
      </div>
    </div>
  </div>
</section>
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Qualitative Results</h3>
        <img src="static/images/figure3.png" alt="Qualitative Performance Comparison on MUSIC dataset" style="width:100%;">
        <h2 class="subtitle">
          Visual comparison of our method (fourth row) with CLIPSep (third row) on the MUSIC dataset. Our approach produces cleaner, more distinct separated sounds.
        </h2>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Quantitative Results</h3>
        <img src="static/images/table1.png" alt="Table 1: Performance on MUSIC dataset">
        <h2 class="subtitle">
          Table 1: Performance comparison on the MUSIC dataset.
        </h2>
        </div>
    </div>
  </div>
</section>
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Full Paper</h2>
      <iframe src="static/pdfs/BMVC_Han_final.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{hu2025audio,
  title={Audio-Visual Separation with Hierarchical Fusion and Representation Alignment},
  author={Hu, Han and Lin, Dongheng and Huang, Qiming and Hou, Yuqi and Chang, Hyung Jin and Jiao, Jianbo},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2025},
}</code></pre>
  </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
  </html>